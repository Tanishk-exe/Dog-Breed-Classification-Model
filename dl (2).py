# -*- coding: utf-8 -*-
"""DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usZ5gPPpvXKv9Jmy1e2qMbNfJTeXRW07
"""

#!unzip "/content/drive/MyDrive/Dataset/dog-breed-identification.zip" -d "/content/drive/MyDrive/Dataset/dog-breed-identification/"

import tensorflow as tf
import pandas as pd
from tensorflow.keras import layers

labels=pd.read_csv('/content/drive/MyDrive/Dataset/dog-breed-identification/labels.csv');
labels.head()

labels.describe()

labels['breed'].value_counts().plot(kind='bar', figsize=(20,12));

labels['breed'].value_counts().median()

from IPython.display import Image
Image("/content/drive/MyDrive/Dataset/dog-breed-identification/train/16ca736a6ff68bc4d2d1586e8eec9b28.jpg")

filename=["/content/drive/MyDrive/Dataset/dog-breed-identification/train/" + fname + ".jpg" for fname in labels['id']]
filename

import os
if len(os.listdir('/content/drive/MyDrive/Dataset/dog-breed-identification/train/')) == len(filename):
  print("Length is same")
else:
  print("No")

len(filename)

print(len(os.listdir('/content/drive/MyDrive/Dataset/dog-breed-identification/test/')))

Image(filename[10221])

labels['breed'][10221]

import numpy as np
lbreed=labels['breed']
lbreed = np.array(lbreed)
lbreed

unique=np.unique(lbreed)

len(unique)

lbreed[0] == unique

bool_label=[label == unique for label in lbreed]
bool_label[:2]

len(bool_label)

bool_label[0].astype(int)

np.where(bool_label[0])

from sklearn.model_selection import train_test_split

x= filename
y=bool_label

num_img= 1000 #@param {type:"slider", min:1000, max:10000, step:100}

x_train, x_val, y_train, y_val=train_test_split(x[:num_img],y[:num_img], test_size=0.2, random_state=5)

len(x_train), len(x_val)

x_train[:5], y_train[:2]

"""# **Preprocessing our images**

converting image into numpy array

To preprocess our images into tensors



1.   Take image as input
2.   use tensorflow to read the files and save it in variable
3.   Turn our image into tensors
4.   resize image into similar shape in this (224,224)
5.   return modified image
"""

from matplotlib.pyplot import imread

image=imread(filename[42])
image

image.shape

"""we have converted array of color values of photo into tensors because although its same but tensors can make these data able to run on gpu which will be very fast for model to run

it makes the image preprocess faster
"""

tf.constant(image)

img_size=224
def process_image(image_path):

  #Reading image file
  image=tf.io.read_file(image_path)

  #turn image into tensor with 3 channels
  image=tf.image.decode_jpeg(image, channels=3)

  #convert color channel from 0-255 into 0-1 values
  #Normalization
  image=tf.image.convert_image_dtype(image, tf.float32)

  #Resize the image
  image=tf.image.resize(image, size=[img_size, img_size])

  return image

"""So to use tensorflow effectively, we need our data in the form of tensor tuple"""

#function for making only 1 tuple
def get_img_label(image_path, label):
  image=process_image(image_path)
  return image, label

process_image(x[42]), tf.constant(y[42])

"""Now turning our data into batches---taking small portion of and calculating/processing

we cannot process all 10000 images at ones it will not fit in our memory so we give 32 image per batch at a time
and aslo can adjust batch size acc to our need
"""

Batch_size=32

def create_batch(x, y=None, batch_size=Batch_size, valid_data=False, test_data=False):
  if test_data:
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x)))
    data_batch=data.map(process_image).batch(Batch_size)
    return data_batch
  elif valid_data:
    print("Validation data")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x), tf.constant(y)))
    data_batch=data.map(get_img_label).batch(Batch_size)
  else:
    print("TRAINING DATA")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x), tf.constant(y)))
    data=data.shuffle(buffer_size=len(x))
    data=data.map(get_img_label)
    data_batch=data.batch(Batch_size)
  return data_batch

train_data= create_batch(x_train, y_train)
val_data=create_batch(x_val, y_val, valid_data=True)

train_data.element_spec

import matplotlib.pyplot as plt

def show_img(images, labels):
  plt.figure(figsize=(12,12))
  for i in range(25):
    ax=plt.subplot(5, 5, i+1)
    plt.imshow(images[i])
    plt.title(unique[labels[i].argmax()])
    plt.axis("off")

train_image, train_label=next(train_data.as_numpy_iterator())

#show_img(train_image, train_label)

#val_images, val_label=next(val_data.as_numpy_iterator())

#show_img(val_images, val_label)

"""Input shape --> our image shape(in the form of tensors) to our model

Output shape --> image labels(in the form of tensors) of our model

The URL of model we want to use
"""

input_shape = [img_size, img_size, 3]

output_shape = len(unique)

model_url="https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"



"""Now creating a function which:


*   Take input shape, output shape and model we have chosen as paramters
*   Define the layers in keras model in sequential manner


*   Compiles the model
*   Build the model(tells the model the imput shape it'll be getting)


*   Returns the model





"""

#!pip install -q tf-keras

import tensorflow_hub as hub
import tf_keras as keras
from tf_keras import layers


def create_model(input_shape=input_shape, output_shape=output_shape, model_url=model_url):

    model = keras.Sequential([
        layers.Input(shape=input_shape),
        hub.KerasLayer(model_url, trainable=False),
        layers.Dense(output_shape, activation="softmax")
    ])

    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        metrics=["accuracy"]
    )
    model.build(input_shape)

    return model


model = create_model()
model.summary()

# Commented out IPython magic to ensure Python compatibility.
#Creating Callbacks using tensorboard

# %load_ext tensorboard

import datetime
from tf_keras.callbacks import TensorBoard, EarlyStopping

def create_log():
    logdir=os.path.join("/content/drive/MyDrive/Dataset/dog-breed-identification/logs")
    datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    return TensorBoard(log_dir=logdir)

"""early stopping hepls stop our model from overfitting by stopping training"""

es=EarlyStopping(monitor="val_accuracy", patience=3)

"""First train our model on 1000 images of data"""

num_epochs=100 #@param {type:"slider", min:10, max:100}

# creating a function which train our model

def train_model():
  model=create_model()

  tensorboard=create_log()

  model.fit(x=train_data,
            epochs=num_epochs,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, es])
  return model

model=train_model()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir "/content/drive/MyDrive/Dataset/dog-breed-identification/logs"

val_data

pred=model.predict(val_data, verbose=1)
pred

pred.shape

pred[0]

np.sum(pred[1])

"""Now finding predictions"""

i=11
print(pred[i])
print(f'Max:{np.max(pred[i])}')
print(f'Sum: {np.sum(pred[i])}')
print(f'Max index: {np.argmax(pred[i])} ')
print(f'{unique[np.argmax(pred[i])]}')

def get_label(pred_prob):
  return unique[np.argmax(pred_prob)]

pred_label=get_label(pred[51])
pred_label

"""Our validation data is in batch dataset so we have to unbatch it to make predictions on validation images and then compare those prediction to validation labels"""

# function to unbatch data

def unbatch_data(data):
  img_=[]
  labels_=[]

  for image, label in val_data.unbatch().as_numpy_iterator():
    img_.append(image)
    labels_.append(unique[np.argmax(label)])

  return img_, labels_

val_img, val_labels=unbatch_data(val_data)

val_img[0], val_labels[0]

get_label(val_labels[0])

def plot_pred(pred_prob, labels, images, n=1):
  pred_prob, true_label, image=pred_prob[n], labels[n], images[n]
  pred_label=get_label(pred_prob)

  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])


  if pred_label == true_label:
    color= "green"
  else:
    color= "red"

  plt.title("{} {:2.0f}% {}".format(pred_label, np.max(pred_prob)*100, true_label), color= color)

plot_pred(pred, val_labels, val_img, 89)

def save_model(model, suffix=None):
  modeldir=os.path.join("/content/drive/MyDrive/Dataset/dog-breed-identification/models/")

  model_path=modeldir + suffix + ".h5"

  print(f'saving model to: {model_path}')

  model.save(model_path)

  return model_path

def load_model(model_path):

    print("Loading saved model")

    model = keras.models.load_model(
        model_path,
        custom_objects={"KerasLayer": hub.KerasLayer}
    )

    return model

save_model(model, suffix="1000-images-mobilenetV2")

loaded_1k=load_model('/content/drive/MyDrive/Dataset/dog-breed-identification/models/1000-images-mobilenetV2.h5')

"""Now training our model on full data

"""

len(x)

len(y)

full_data=create_batch(x,y)

full_data

full_model=create_model()

f_m_tb=create_log()

f_m_es=EarlyStopping(monitor="accuracy", patience=3)

"""Fitting our model to 1000+ images of data"""

full_model.fit(x=full_data, epochs=num_epochs, callbacks=[f_m_tb, f_m_es] )

save_model(full_model, suffix="Full-model")

loaded_model=load_model('/content/drive/MyDrive/Dataset/dog-breed-identification/models/Full-model.h5')

"""Now testing our model on our test data"""

test_path="/content/drive/MyDrive/Dataset/dog-breed-identification/test/"

filenames=[test_path + fname for fname in os.listdir(test_path)]
filenames[:10]

len(filenames)

test_data=create_batch(filenames, test_data=True)

test_data

test_pred=loaded_model.predict(test_data, verbose=1)